{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dafef955-4c2c-a871-f1d8-3e0d306393b0"
   },
   "source": [
    "# Breast cancer diagnostic classification exercise\n",
    "\n",
    "- 0/1로 구분하는 task\n",
    "- Logistic Regression\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e26372e-f1bd-b50f-0c1c-33a44306d1f7"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2768ce80-1a7d-ca31-a35f-29cf0ef7fb15"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "#import mpld3 as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09b9d090-2cba-ad5a-58ce-84208f95dba4"
   },
   "source": [
    "# 1. Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9180cb22-53d2-6bf2-3a29-99448ab808fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38           122.8     1001.0   \n",
       "1    842517         M        20.57         17.77           132.9     1326.0   \n",
       "2  84300903         M        19.69         21.25           130.0     1203.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33            184.6      2019.0            0.1622   \n",
       "1  ...          23.41            158.8      1956.0            0.1238   \n",
       "2  ...          25.53            152.5      1709.0            0.1444   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/breast_cancer.csv\",header = 0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "---\n",
    "### 1. Preprocessing:\n",
    "\n",
    "- 1.1 Drop the \"id\" column.\n",
    "- 1.2 Replace the label \"diagnosis\" with binary labels, where \"M\"(Malignant)=1, and \"B\"(=Benign)=0.\n",
    "- 1.3 Split the dataset into a training set(=20% of the data) and a test set(=20% of the data). Shuffle the data.\n",
    "- 1.4 For both datasets, split it into a dataset with only features: \"X\" and one another containing only the labels: \"Y\".\n",
    "\n",
    "### 2. Implementation and training of the models:\n",
    "- 2.1 Implement and train a Logistic Regression model on the training set using SkLearn. Use the default arguments for the model. \n",
    "- 2.2 Implementand train a Decision Tree model on the training set using SkLearn. Use the default arguments for the model.\n",
    "- 2.3 Evaluate the test accuracy of both models on the test set, which one is better ?\n",
    "- 2.4 Are we overfitting for one of the models ? \n",
    "\n",
    "### 3. Playing with different features and Hyperparameters\n",
    "- 3.1 Train both the Logistic Regression AND the Decision Tree model using only the following features:\n",
    "> `radius_mean`,`perimeter_mean`,`area_mean`,`compactness_mean`,`concave points_mean`\n",
    "\n",
    "- 3.2 Compare the test accuracy results with the models from 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess the data\n",
    "- [df.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)\n",
    "    - inplace는 대체할 것인지 안할 것인지 결정, False가 디폴트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "f9fd3701-af9d-8d8c-5d0e-e2673d7977fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1\n",
    "df.drop('id',axis=1, inplace=True)\n",
    "df.drop('Unnamed: 32',axis=1, inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map함수 이용하여 속성값 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "083fe464-8dac-713e-d0a1-46435c0d93fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38           122.8     1001.0   \n",
       "1          1        20.57         17.77           132.9     1326.0   \n",
       "2          1        19.69         21.25           130.0     1203.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33            184.6   \n",
       "1         0.1812  ...         24.99          23.41            158.8   \n",
       "2         0.2069  ...         23.57          25.53            152.5   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2\n",
    "df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0882e4c2-3d4d-d4d9-5f49-f36c1b248b93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.3\n",
    "trainset, testset = train_test_split(df, test_size = 0.2, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "1390898b-a338-7395-635f-6e1c216861e6"
   },
   "outputs": [],
   "source": [
    "# 1.4\n",
    "Y_train = trainset['diagnosis'] # labels\n",
    "X_train = trainset.drop('diagnosis',axis=1) # features\n",
    "# do same thing with test dataset\n",
    "Y_test = testset['diagnosis']\n",
    "X_test = testset.drop('diagnosis',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.05</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.04304</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.30</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>11.13</td>\n",
       "      <td>16.62</td>\n",
       "      <td>70.47</td>\n",
       "      <td>381.1</td>\n",
       "      <td>0.08151</td>\n",
       "      <td>0.03834</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.01370</td>\n",
       "      <td>0.1511</td>\n",
       "      <td>0.06148</td>\n",
       "      <td>...</td>\n",
       "      <td>11.68</td>\n",
       "      <td>20.29</td>\n",
       "      <td>74.35</td>\n",
       "      <td>421.1</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.06219</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.04044</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.07083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>19.18</td>\n",
       "      <td>22.49</td>\n",
       "      <td>127.50</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>0.08523</td>\n",
       "      <td>0.14280</td>\n",
       "      <td>0.11140</td>\n",
       "      <td>0.06772</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>0.05529</td>\n",
       "      <td>...</td>\n",
       "      <td>23.36</td>\n",
       "      <td>32.06</td>\n",
       "      <td>166.40</td>\n",
       "      <td>1688.0</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>0.56010</td>\n",
       "      <td>0.3865</td>\n",
       "      <td>0.17080</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>0.09221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "560        14.05         27.15           91.38      600.4          0.09929   \n",
       "428        11.13         16.62           70.47      381.1          0.08151   \n",
       "198        19.18         22.49          127.50     1148.0          0.08523   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "560           0.11260         0.04462              0.04304         0.1537   \n",
       "428           0.03834         0.01369              0.01370         0.1511   \n",
       "198           0.14280         0.11140              0.06772         0.1767   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "560                 0.06171  ...         15.30          33.17   \n",
       "428                 0.06148  ...         11.68          20.29   \n",
       "198                 0.05529  ...         23.36          32.06   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "560           100.20       706.7            0.1241            0.22640   \n",
       "428            74.35       421.1            0.1030            0.06219   \n",
       "198           166.40      1688.0            0.1322            0.56010   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "560           0.1326               0.10480          0.2250   \n",
       "428           0.0458               0.04044          0.2383   \n",
       "198           0.3865               0.17080          0.3193   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "560                  0.08321  \n",
       "428                  0.07083  \n",
       "198                  0.09221  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560    0\n",
       "428    0\n",
       "198    1\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "45dac047-52fb-b847-a521-fa6883ebd5f6"
   },
   "source": [
    "### 2. Implement, train, and evaluate the models\n",
    "- 로지스틱 회귀 모델과 의사결정트리 모델 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr=LogisticRegression(max_iter=1000000)\n",
    "model_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 96.923%\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "train_predictions_lr = model_lr.predict(X_train)\n",
    "# metrics는 sklearn에서 가져온 모듈\n",
    "train_accuracy_lr = metrics.accuracy_score(train_predictions_lr, Y_train)\n",
    "\n",
    "print(\"Train Accuracy : %s\" % \"{0:.3%}\".format(train_accuracy_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9692307692307692"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 92.982%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "test_predictions_lr = model_lr.predict(X_test)\n",
    "test_accuracy_lr = metrics.accuracy_score(test_predictions_lr, Y_test)\n",
    "print(\"Test accuracy : %s\" % \"{0:.3%}\".format(test_accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅은 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 100.000%\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "train_predictions_dt = model_dt.predict(X_train)\n",
    "train_accuracy_dt = metrics.accuracy_score(train_predictions_dt, Y_train)\n",
    "\n",
    "print(\"Accuracy : %s\" % \"{0:.3%}\".format(train_accuracy_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 100.000%\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "train_predictions_dt = model_dt.predict(X_train)\n",
    "train_accuracy_dt = metrics.accuracy_score(train_predictions_dt,Y_train)\n",
    "\n",
    "print(\"Accuracy : %s\" % \"{0:.3%}\".format(train_accuracy_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train on different features\n",
    "- input이 되는 X(feature)들 조정해서 training 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr2=LogisticRegression(max_iter=10000000)\n",
    "model_lr2.fit(X_train[features], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 90.330%\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "train_predictions_lr2 = model_lr2.predict(X_train[features])\n",
    "train_accuracy_lr2 = metrics.accuracy_score(train_predictions_lr2,Y_train)\n",
    "print(\"Train Accuracy : %s\" % \"{0:.3%}\".format(train_accuracy_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 87.719%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "test_predictions_lr2 = model_lr2.predict(X_test[features])\n",
    "test_accuracy_lr2 = metrics.accuracy_score(test_predictions_lr2, Y_test)\n",
    "print(\"Test accuracy : %s\" % \"{0:.3%}\".format(test_accuracy_lr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt2 = DecisionTreeClassifier()\n",
    "model_dt2.fit(X_train[features], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 100.000%\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "train_predictions_dt2 = model_dt2.predict(X_train[features])\n",
    "train_accuracy_dt2 = metrics.accuracy_score(train_predictions_dt2, Y_train)\n",
    "print(\"Accuracy : %s\" % \"{0:.3%}\".format(train_accuracy_dt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 87.719%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "test_predictions_dt2 = model_dt2.predict(X_test[features])\n",
    "test_accuracy_dt2 = metrics.accuracy_score(test_predictions_dt2,Y_test)\n",
    "print(\"Test accuracy : %s\" % \"{0:.3%}\".format(test_accuracy_dt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 4.: Write own logistic regression class\n",
    "\n",
    "Write a class that has the following methods:\n",
    "\n",
    "- `fit(X,Y)`: Should fit the model weights using feature matrix X and label vector y. Implement gradient descent to find a solution for a suitable w-vector.\n",
    "\n",
    "- `predict(X)`: Should make predictions label predictions for X.\n",
    "\n",
    "- `score(X,Y)`: Should make label predictions on X, and compute accuracy using the label information Y and the predictions.\n",
    "\n",
    "- train and evaluate your model on the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import log_loss \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((X.shape[0], 1)), X]) # bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Gradient:\n",
    "---\n",
    "$\\nabla_{\\mathbf w} J(\\mathbf w) = \\frac{1}{N}\\mathbf X^T(\\sigma(\\mathbf X \\mathbf w)-\\mathbf y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sigmoid function - Warning: Might encounter overflow error\n",
    "def sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy의 [expit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "# Expit (a.k.a. logistic sigmoid) ufunc for ndarrays.\n",
    "# The expit function, also known as the logistic sigmoid function, is defined as expit(x) = 1/(1+exp(-x)). \n",
    "# It is the inverse of the logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Binary Logistic Regressor that uses Gradient Descent for learning the best fitting weights\n",
    "class BinaryLogisticRegressor():\n",
    "    \n",
    "    # Should store the positive class and its weights\n",
    "    def __init__(self, pos_class=None):\n",
    "        self.w = None\n",
    "        self.pos_class = pos_class\n",
    "    \n",
    "    # Implement GD using The formula above\n",
    "    def fit(self, X, Y, n_epochs=1000, alpha=0.001):\n",
    "        self.w = np.random.rand(X.shape[1],1)\n",
    "        loss = []\n",
    "        \n",
    "        for i in range(n_epochs):        \n",
    "            # Compute the gradient\n",
    "            # scipy의 expit 이용\n",
    "            delta_J = (1/len(X)) * np.dot(X.T, (expit(np.dot(X,self.w)) - Y[:,np.newaxis]))\n",
    "            \n",
    "            # Do the update\n",
    "            self.w = self.w - alpha * delta_J\n",
    "            \n",
    "            # store the lossin loss array (optional)\n",
    "            loss.append(log_loss(Y, expit(np.dot(X,self.w))))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = expit(np.dot(X,self.w))\n",
    "        return np.round(predictions)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy_score = np.sum(predictions == Y[:,np.newaxis]) / X.shape[0]\n",
    "        \n",
    "        return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_class_dataset(X, Y, pos_class_id):\n",
    "    # 해당하는 클래스가 이다./아니다. 로 라벨 만들기\n",
    "    \n",
    "        class_1_ids = np.where(Y == pos_class_id)[0]\n",
    "        class_0_ids = np.where(Y != pos_class_id)[0]\n",
    "        \n",
    "        X_1 = X[class_1_ids]\n",
    "        X_0 = X[class_0_ids]\n",
    "        \n",
    "        Y_1 = np.ones(len(Y[class_1_ids])) \n",
    "        Y_0 = np.zeros(len(Y[class_0_ids]))\n",
    "    \n",
    "        X = np.concatenate([X_1, X_0], axis=0)\n",
    "        Y = np.concatenate([Y_1, Y_0])\n",
    "    \n",
    "        shuffled_indices = np.random.permutation(X.shape[0])\n",
    "        return X[shuffled_indices], Y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "멀티클래스 로지스틱 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = None\n",
    "        \n",
    "    def fit(self, X, Y, n_epochs=2):\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        # 1. Get k unique labels\n",
    "        # 2  Create array to store k model weights \n",
    "        # 3. Get unique, class-dependent datasets\n",
    "        # 4. For each unique class:\n",
    "            # Create a binary dataset\n",
    "            # Create binary classifier\n",
    "            # Train binary classifier\n",
    "        # 5. Store model weights\n",
    "        \n",
    "        self.num_features = X.shape[1]\n",
    "        # 1\n",
    "        self.unique_classes = np.unique(Y)\n",
    "        # 2\n",
    "        # Create array to store model weights\n",
    "        self.models = np.zeros((len(self.unique_classes), self.num_features))\n",
    "        \n",
    "        # 3\n",
    "        for i, pos_cls in enumerate(self.unique_classes):\n",
    "            # 4\n",
    "            # Create unique dataset\n",
    "            # 위에서 정의한 데이터셋 가져오는 함수 (이다./아니다.) 바이너리\n",
    "            X_tmp, Y_tmp = get_2_class_dataset(X, Y, pos_cls)\n",
    "            \n",
    "            # Create binary classifier\n",
    "            tmp_model = BinaryLogisticRegressor(pos_cls)\n",
    "            \n",
    "            # Train binary classifier\n",
    "            tmp_model.fit(X_tmp, Y_tmp, n_epochs)\n",
    "            \n",
    "            # 5\n",
    "            # Store model weights 모델 저장\n",
    "            self.models[i][:,np.newaxis] += tmp_model.w[:]\n",
    "            \n",
    "    def predict(self, X):\n",
    "\n",
    "        # Create empty prediction table:\n",
    "        predictions = np.zeros((len(self.unique_classes), 2, X.shape[0]))\n",
    "        # First Dimension represents classifiers\n",
    "        # Second Dimension represents points\n",
    "        # Third Dimension represents class scores, first entry is prob. score for class 1, second class 0\n",
    "        \n",
    "        # Do a prediction for every classifier\n",
    "        # 모델 불러오기\n",
    "        for i, weights in enumerate(self.models):\n",
    "            \n",
    "            # Probability score for class 1\n",
    "            predictions[i][0][:,np.newaxis] += expit(np.dot(X, weights[:,np.newaxis]))\n",
    "            \n",
    "            # Probability score for class 2\n",
    "            predictions[i][1][:,np.newaxis] += 1 - expit(np.dot(X, weights[:,np.newaxis]))\n",
    "            \n",
    "        # final predictions are is the argmax over classifier probability scores for class 1\n",
    "        final_predictions = np.argmax(predictions[:,0,:], axis=0)\n",
    "        \n",
    "        return final_predictions\n",
    "            \n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        correctly_predicted = np.sum(predictions == Y) \n",
    "        return correctly_predicted / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5674157303370787"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만든 MultiClassLogisticRegression 확인해보기\n",
    "mclr = MultiClassLogisticRegression()\n",
    "mclr.fit(X,Y, 3000)\n",
    "preds = mclr.predict(X)\n",
    "mclr.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 14)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mclr.models.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 416,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
