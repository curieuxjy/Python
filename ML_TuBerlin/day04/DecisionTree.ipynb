{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem setting\n",
    "Find the relationship between the features of a data point and it's corresponding class label. For a binary decision tree we only have 2 class labels, such as pass/fail, win/lose, alive/dead, healthy/sick, 0/1 etc. So given the features of a data point, the binary decision tree model should output/predict a probability value, for this point belonging to class 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data set and objective:\n",
    "---\n",
    "![image.png](resources/dt_dataset.png)\n",
    "\n",
    "### Objective: \n",
    "\n",
    "Find feature splits that perfectly split all the classes in the leaf nodes of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic algorithm:\n",
    "1. Define metric that measures the \"importance\" of splitting at a certain feature\n",
    "2. Apply this metric at every feature and select the one with the highest value\n",
    "3. If tree leaf nodes aren't perfectly distinguished by label, repeat step 2.\n",
    "\n",
    "### Note: The Decision tree algorithm is easy exentible to more than 2 classes. No changes have to be made, as long as the leaf nodes contain only 1 class after the tree has been built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which metric to choose that measures importance of a feature ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information Gain\n",
    "---\n",
    "The formula for information gain is the following:\n",
    "$Information\\_Gain(\\text{feature a},Y)=H(Y)-H(Y|\\text{feature a})$, or\n",
    "\n",
    "$Information\\_Gain$\n",
    "$=(\\text{Entropy of distribution before the splitting})$\n",
    "$-(\\text{Entropy of distribution after observing the feature value of feature a})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy of 1 random variable\n",
    "For the discrete case, the Entropy of a random variable is defined as: $H(\\mathbf Y)=-\\sum_{i=1}^{K}P(Y_i) log_2(P(Y_i))$, with $Y$ being the random variable of classes, $K$ being the number of different classes and $P(Y_i)$ being the probability(in this case relative frequency) of class $i$. It is a measure of homogeneity, or information content(or disorder, randomness) of a probability distribution of a random variable. So for example when taking the label play golf as our random variable, with it's class frequencies as a discrete probability distribution, we get the following entropy value\n",
    "$Entropy:=H(\\mathbf Y)=-\\frac{4}{6}log_2(\\frac{4}{6})-\\frac{2}{6}log_2(\\frac{2}{6})=0.918$  \n",
    "\n",
    "![](resources/h.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Entropy of the tree above: 4 out of 6 labels are yes, 2 out of 6 are no\n",
    "label_entropy = -(4/6)*np.log2((4/6))-(2/6)*np.log2((2/6))\n",
    "print(label_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1222915970693747"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Say 59 of 60 labels are \"yes\"\n",
    "-(59/60)*np.log2((59/60))-(1/60)*np.log2((1/60)) # <- low entropy, meaning low information content(randomness) in the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3: Say 30 of 60 labels are \"yes\"\n",
    "-(30/60)*np.log2((30/60))-(30/60)*np.log2((30/60)) # <- high entropy, meaning high information content(randomness) in the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional entropy of a random variable, given a distinct feature value:\n",
    "Definition of conditional entropy:\n",
    "$H(\\mathbf Y| \\text{feature a})=\\sum_{v\\epsilon vals(a)}P_a(v)H(S_a(v))$, \n",
    "\n",
    "where $a$ is a specific feature(e.g. \"Outlook\"), $S_a(v)=\\{x\\epsilon D|x_a=v\\}$ is the set of data points in $D$, whose feature value for feature $a$ is equal to $v$, $vals(a)$ is the set of unique elements of feature $a$, $P_a(v)$ is the probability of feature $a$, having the value $v$, or $P_a(v)=\\frac{|S_a(v)|}{|D|}$, and $H(S_a(v))$ is the entropy over the labels induced by the set $S_a(v)$. \n",
    "\n",
    "In information theory, the conditional entropy quantifies the amount of information needed to describe the outcome of a random variable $Y$ given that the value of another random variable $\\text{feature a}$ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Joint entropy label and feature 1:\n",
    "Let $X$ be the variable for feature \"Outlook\".\n",
    "![](resources/d1.png)\n",
    "$H(\\mathbf Y|Outlook)$\n",
    "=$P_{Outlook}(sunny)\\cdot H(S_{Outlook}(sunny))$\n",
    "\n",
    "$+P_{Outlook}(rainy)\\cdot H(S_{Outlook}(rainy))$\n",
    "\n",
    "$+P_{Outlook}(cloudy)\\cdot H(S_{Outlook}(cloudy))$\n",
    "\n",
    "$=\\frac{2}{6}(-\\frac{2}{2}log_2(\\frac{2}{2})-\\frac{0}{2}log_2(\\frac{0}{2}))+\\frac{2}{6}(-\\frac{1}{2}log_2(\\frac{1}{2})-\\frac{1}{2}log_2(\\frac{1}{2}))+\\frac{2}{6}(-\\frac{1}{2}log_2(\\frac{1}{2})-\\frac{1}{2}log_2(\\frac{1}{2}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_feature_1 = (2/6)*(-0.5*np.log2(0.5)-0.5*np.log2(0.5))+(2/6)*(-0.5*np.log2(0.5)-0.5*np.log2(0.5))\n",
    "print(joint_entropy_feature_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Joint entropy label and feature 2:\n",
    "Let $X$ be the random variable for feature \"Temperature\".\n",
    "![](resources/d2.png)\n",
    "$H(\\mathbf Y|Temperature)\n",
    "=P_{Temperature}(cool)\\cdot H(S_{Temperature}(cool))$\n",
    "\n",
    "$+P_{Temperature}(hot)\\cdot H(S_{Temperature}(hot))$\n",
    "\n",
    "$=\\frac{2}{6}(-\\frac{2}{2}log_2(\\frac{2}{2})-\\frac{0}{2}log_2(\\frac{0}{2}))+\\frac{4}{6}(-\\frac{2}{4}log_2(\\frac{2}{4})-\\frac{2}{4}log_2(\\frac{2}{4}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_feature_2 = (4/6)*(-2*(2/4)*np.log2((2/4)))\n",
    "\n",
    "print(joint_entropy_feature_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Joint entropy label and feature 3:\n",
    "Let $X$ be the random variable for feature \"Wind\".\n",
    "![](resources/d3.png)\n",
    "$H(\\mathbf Y|Wind)$\n",
    "\n",
    "$=P_{Wind}(strong)\\cdot H(S_{Wind}(strong))+P_{Wind}(weak)\\cdot H(S_{Wind}(weak))$\n",
    "\n",
    "$=\\frac{3}{6}(-\\frac{1}{3}log_2(\\frac{1}{3})-\\frac{2}{3}log_2(\\frac{2}{3}))+\\frac{3}{6}(-\\frac{3}{3}log_2(\\frac{3}{3})-\\frac{0}{3}log_2(\\frac{0}{3}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591479170272448\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_feature_3 = (3/6)*(-(1/3)*np.log2((1/3))-(2/3)*np.log2((2/3)))\n",
    "print(joint_entropy_feature_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information Gain\n",
    "Next, we compute the Information gain:\n",
    "\n",
    "$Information\\_Gain(\\text{feature a},Y)=H(Y)-H(Y|\\text{feature a})$, \n",
    "\n",
    "using the label Entropy $H(Y)$ and the conditional Entropy of the label random variable for each $\\text{feature a}$, one by one. \n",
    "\n",
    "$Information\\_Gain=(\\text{Entropy of distribution before the splitting})-(\\text{Entropy of distribution after observing the feature value of feature a})$\n",
    "https://en.wikipedia.org/wiki/Information_gain_in_decision_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2516291673878229, 0.2516291673878229, 0.4591479170272448)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain_feat_1 = label_entropy - conditional_entropy_feature_1  \n",
    "information_gain_feat_2 = label_entropy - conditional_entropy_feature_2\n",
    "information_gain_feat_3 = label_entropy - conditional_entropy_feature_3  \n",
    "\n",
    "information_gain_feat_1, information_gain_feat_2, information_gain_feat_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### $\\rightarrow$ Use feature 3 as splitting node, and repeat the process with the remaining data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Gini Index:\n",
    "An alternative metric to measure on what feature to split the tree.\n",
    "https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train a Decision Tree using SkLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 0. Do the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
