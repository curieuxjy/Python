{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6 - Feature Engineering and Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6\n",
    "--- \n",
    "1. Bias-variance tradeoff\n",
    "2. Regularization (Lasso, Ridge, Early Stopping)\n",
    "3. Feature Engineering\n",
    "  1. Vectorization and Standardization\n",
    "  2. Feature selection\n",
    "  3. Feature extraction/creation\n",
    "4. Model Selection\n",
    "  1. Cross Validation\n",
    "  2. GridSearch & RandomSearch\n",
    "  3. score metrics (Classification/Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Bias-variance tradeoff\n",
    "---\n",
    "The errors of a machine learning models can be separated into:\n",
    " - reducible error\n",
    " - irreducible error\n",
    "\n",
    "Irreducible errors can not be changed, even when using different models. \n",
    "\n",
    "The reducible error can be decomposed into **bias and variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias\n",
    "Let $\\hat\\theta = g(x_1,...,x_N)$ be a point estimator of the true $\\theta$ that generated the data.\n",
    "\n",
    "---\n",
    "Bias: (inherent) inability of a model to capture relation between data and parameters.\n",
    " - Bias: $\\mathbb{E}_{x|\\theta}[\\hat\\theta] - \\theta$,\n",
    "\n",
    "Example: Linear regression model has inherent bias of assuming linear relationship between X and y. In reality, there may be additional unknown- and/or non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance\n",
    "---\n",
    "Variance: how much does our estimator change as function of the data sample. I.e. how much does it overfit a particular dataset.\n",
    " - Variance: $Var[\\hat\\theta] = \\mathbb{E}[\\hat\\theta - \\mathbb{E}[\\hat\\theta]^2]$\n",
    "\n",
    "Example: Decision tree with too high depth. Neural Network with too many Hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img\\ufof.png\" alt=\"Drawing\" style=\"width: 1024px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-variance tradeoff \n",
    "**=> Generalization performance**\n",
    "\n",
    "---\n",
    "We can't do anything about the irreducible error.\n",
    "The reducible error can be decomposed into **bias and variance**.\n",
    "\n",
    "Applying such a decomposition for the Mean Squared error loss, it can be shown that\n",
    " - $Err[(y - \\hat f(x;D))^2] =  Bias(\\hat f(x))^2 + Var(\\hat f(x))$\n",
    "\n",
    "**Cross validation can be used to optimize the bias-variance tradeoff of your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underfitting/Overfitting\n",
    "---\n",
    "<img src=\"img\\bias-variance-tradeoff.png\" alt=\"Drawing\" style=\"width: 1024px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model improvements\n",
    "---\n",
    "Techniques to decrease Variance (reduce overfitting):\n",
    " - dimensionality reduction\n",
    " - feature selection\n",
    " - regularization\n",
    " - more data\n",
    " - ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regularization\n",
    "---\n",
    " 1. Lasso\n",
    " 2. Ridge\n",
    " 3. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img\\bias-variance-tradeoff.png\" alt=\"Drawing\" style=\"width: 1024px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regularization\n",
    "### 1. L1 - Regularization (Lasso)\n",
    "---\n",
    "\n",
    "Penalizes the model by the l1-norm of its coefficients by adding the term\n",
    " - $\\lambda \\sum^p_{j=1} |w_j|$\n",
    "\n",
    "to the objective function.\n",
    " \n",
    "Higher values for $\\lambda$\n",
    " => more coefficients close to or equal to zero \n",
    " => more features irrelevant for computing the target\n",
    "\n",
    "**=> L1 regularization can be used for feature selection!**\n",
    " - e.g. remove features $j$, for which corresponding weight $w_j$ is zero\n",
    " - then train other models with reduced feature set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regularization\n",
    "### 1. L1 - Regularization (Lasso)\n",
    "---\n",
    "\n",
    "For example for regression\n",
    " - minimize $\\sum^n_{i=1}(y_i - \\sum^n_{j=1}x_{ij}w_j)^2 + \\lambda \\sum^p_{j=1} |w_j|$ \n",
    " \n",
    "or for classification (via L1-SVMs)\n",
    " - minimize $| \\mathbf{w} | + C | \\xi |$ subject to\n",
    " $D(A\\mathbf{w} + \\mathbf{1}b) +\\xi \\geq 1$,\n",
    " $\\xi \\geq 1$\n",
    " \n",
    " \n",
    "As $\\lambda$ increases, bias __ creases\n",
    "\n",
    "As $\\lambda$ increases, variance __ creases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regularization\n",
    "### 2. L2 - Regularization (Ridge)\n",
    "---\n",
    "\n",
    "Penalizes the model by the l2-norm of its coefficients by adding the term\n",
    " - $\\lambda \\sum^p_{j=1} \\lVert w_j \\rVert^2_2$\n",
    "\n",
    "Examples from $L1$ - Regularization apply analogously. \n",
    "\n",
    "However, $L2$ - Regularization is not used for feature selection!\n",
    " - because square penalty does not enforce sparsity of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regularization\n",
    "### 3. Early Stopping\n",
    "---\n",
    "Another, less explicit, regularization technique\n",
    "\n",
    "**Early Stopping = Only train for a limited number of iterations!**\n",
    "\n",
    "Examples:\n",
    "\n",
    " - Decision Trees will have less depth\n",
    " - Neural Networks with have less memorization of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Feature Engineering\n",
    "---\n",
    "1. Vectorization and Standardization\n",
    " - numerical stability\n",
    " - better training results due to improved geometry of the problem\n",
    "2. Feature extraction/creation\n",
    " - introduce potential feature relations\n",
    "3. Feature selection\n",
    " - interpretability\n",
    " - shorter training times\n",
    " - enhanced generalization by reducing overfitting/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Feature Engineering\n",
    "---\n",
    "\n",
    "The process of creating features for machine learning from raw data is what we refer to as feature engineering.\n",
    "\n",
    "Lets get practical with something just as interesting: Speed-dating!\n",
    "On the speeddating dataset we will investigate the following instances of Feature Engineering:\n",
    "\n",
    " - Data Imputation\n",
    " - Standardization\n",
    " - Vectorization of categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>wave</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>d_age</th>\n",
       "      <th>d_d_age</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>samerace</th>\n",
       "      <th>...</th>\n",
       "      <th>d_expected_num_interested_in_me</th>\n",
       "      <th>d_expected_num_matches</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>d_like</th>\n",
       "      <th>d_guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>decision</th>\n",
       "      <th>decision_o</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>[4-6]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>?</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Latino/Hispanic American'</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_null  wave  gender age age_o  d_age d_d_age  \\\n",
       "0         0     1  female  21    27      6   [4-6]   \n",
       "1         0     1  female  21    22      1   [0-1]   \n",
       "2         1     1  female  21    22      1   [0-1]   \n",
       "3         0     1  female  21    23      2   [2-3]   \n",
       "4         0     1  female  21    24      3   [2-3]   \n",
       "\n",
       "                                      race  \\\n",
       "0  'Asian/Pacific Islander/Asian-American'   \n",
       "1  'Asian/Pacific Islander/Asian-American'   \n",
       "2  'Asian/Pacific Islander/Asian-American'   \n",
       "3  'Asian/Pacific Islander/Asian-American'   \n",
       "4  'Asian/Pacific Islander/Asian-American'   \n",
       "\n",
       "                                    race_o  samerace  ...  \\\n",
       "0              European/Caucasian-American         0  ...   \n",
       "1              European/Caucasian-American         0  ...   \n",
       "2  'Asian/Pacific Islander/Asian-American'         1  ...   \n",
       "3              European/Caucasian-American         0  ...   \n",
       "4               'Latino/Hispanic American'         0  ...   \n",
       "\n",
       "  d_expected_num_interested_in_me d_expected_num_matches like  \\\n",
       "0                           [0-3]                  [3-5]    7   \n",
       "1                           [0-3]                  [3-5]    7   \n",
       "2                           [0-3]                  [3-5]    7   \n",
       "3                           [0-3]                  [3-5]    7   \n",
       "4                           [0-3]                  [3-5]    6   \n",
       "\n",
       "  guess_prob_liked d_like d_guess_prob_liked met decision decision_o match  \n",
       "0                6  [6-8]              [5-6]   0        1          0     0  \n",
       "1                5  [6-8]              [5-6]   1        1          0     0  \n",
       "2                ?  [6-8]              [0-4]   1        1          1     1  \n",
       "3                6  [6-8]              [5-6]   0        1          1     1  \n",
       "4                6  [6-8]              [5-6]   0        1          1     1  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/speeddating.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>d_d_age</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>d_importance_same_race</th>\n",
       "      <th>d_importance_same_religion</th>\n",
       "      <th>...</th>\n",
       "      <th>expected_num_interested_in_me</th>\n",
       "      <th>expected_num_matches</th>\n",
       "      <th>d_expected_happy_with_sd_people</th>\n",
       "      <th>d_expected_num_interested_in_me</th>\n",
       "      <th>d_expected_num_matches</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>d_like</th>\n",
       "      <th>d_guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>[4-6]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>?</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Latino/Hispanic American'</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender age age_o d_d_age                                     race  \\\n",
       "0  female  21    27   [4-6]  'Asian/Pacific Islander/Asian-American'   \n",
       "1  female  21    22   [0-1]  'Asian/Pacific Islander/Asian-American'   \n",
       "2  female  21    22   [0-1]  'Asian/Pacific Islander/Asian-American'   \n",
       "3  female  21    23   [2-3]  'Asian/Pacific Islander/Asian-American'   \n",
       "4  female  21    24   [2-3]  'Asian/Pacific Islander/Asian-American'   \n",
       "\n",
       "                                    race_o importance_same_race  \\\n",
       "0              European/Caucasian-American                    2   \n",
       "1              European/Caucasian-American                    2   \n",
       "2  'Asian/Pacific Islander/Asian-American'                    2   \n",
       "3              European/Caucasian-American                    2   \n",
       "4               'Latino/Hispanic American'                    2   \n",
       "\n",
       "  importance_same_religion d_importance_same_race d_importance_same_religion  \\\n",
       "0                        4                  [2-5]                      [2-5]   \n",
       "1                        4                  [2-5]                      [2-5]   \n",
       "2                        4                  [2-5]                      [2-5]   \n",
       "3                        4                  [2-5]                      [2-5]   \n",
       "4                        4                  [2-5]                      [2-5]   \n",
       "\n",
       "   ... expected_num_interested_in_me expected_num_matches  \\\n",
       "0  ...                             2                    4   \n",
       "1  ...                             2                    4   \n",
       "2  ...                             2                    4   \n",
       "3  ...                             2                    4   \n",
       "4  ...                             2                    4   \n",
       "\n",
       "  d_expected_happy_with_sd_people d_expected_num_interested_in_me  \\\n",
       "0                           [0-4]                           [0-3]   \n",
       "1                           [0-4]                           [0-3]   \n",
       "2                           [0-4]                           [0-3]   \n",
       "3                           [0-4]                           [0-3]   \n",
       "4                           [0-4]                           [0-3]   \n",
       "\n",
       "  d_expected_num_matches like guess_prob_liked d_like d_guess_prob_liked met  \n",
       "0                  [3-5]    7                6  [6-8]              [5-6]   0  \n",
       "1                  [3-5]    7                5  [6-8]              [5-6]   1  \n",
       "2                  [3-5]    7                ?  [6-8]              [0-4]   1  \n",
       "3                  [3-5]    7                6  [6-8]              [5-6]   0  \n",
       "4                  [3-5]    6                6  [6-8]              [5-6]   0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# realize that numbers are stored as strings...\n",
    "df.select_dtypes(exclude=int).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>d_d_age</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>d_importance_same_race</th>\n",
       "      <th>d_importance_same_religion</th>\n",
       "      <th>field</th>\n",
       "      <th>d_pref_o_attractive</th>\n",
       "      <th>d_pref_o_sincere</th>\n",
       "      <th>d_pref_o_intelligence</th>\n",
       "      <th>...</th>\n",
       "      <th>d_concerts</th>\n",
       "      <th>d_music</th>\n",
       "      <th>d_shopping</th>\n",
       "      <th>d_yoga</th>\n",
       "      <th>d_interests_correlate</th>\n",
       "      <th>d_expected_happy_with_sd_people</th>\n",
       "      <th>d_expected_num_interested_in_me</th>\n",
       "      <th>d_expected_num_matches</th>\n",
       "      <th>d_like</th>\n",
       "      <th>d_guess_prob_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>[4-6]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>Law</td>\n",
       "      <td>[21-100]</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-5]</td>\n",
       "      <td>[0-0.33]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>Law</td>\n",
       "      <td>[21-100]</td>\n",
       "      <td>[0-15]</td>\n",
       "      <td>[0-15]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-5]</td>\n",
       "      <td>[0.33-1]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>[0-1]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>Law</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-5]</td>\n",
       "      <td>[0-0.33]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>Law</td>\n",
       "      <td>[21-100]</td>\n",
       "      <td>[0-15]</td>\n",
       "      <td>[0-15]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-5]</td>\n",
       "      <td>[0.33-1]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>'Latino/Hispanic American'</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>[2-5]</td>\n",
       "      <td>Law</td>\n",
       "      <td>[21-100]</td>\n",
       "      <td>[0-15]</td>\n",
       "      <td>[16-20]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[9-10]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[0-5]</td>\n",
       "      <td>[0-0.33]</td>\n",
       "      <td>[0-4]</td>\n",
       "      <td>[0-3]</td>\n",
       "      <td>[3-5]</td>\n",
       "      <td>[6-8]</td>\n",
       "      <td>[5-6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender d_d_age                                     race  \\\n",
       "0  female   [4-6]  'Asian/Pacific Islander/Asian-American'   \n",
       "1  female   [0-1]  'Asian/Pacific Islander/Asian-American'   \n",
       "2  female   [0-1]  'Asian/Pacific Islander/Asian-American'   \n",
       "3  female   [2-3]  'Asian/Pacific Islander/Asian-American'   \n",
       "4  female   [2-3]  'Asian/Pacific Islander/Asian-American'   \n",
       "\n",
       "                                    race_o d_importance_same_race  \\\n",
       "0              European/Caucasian-American                  [2-5]   \n",
       "1              European/Caucasian-American                  [2-5]   \n",
       "2  'Asian/Pacific Islander/Asian-American'                  [2-5]   \n",
       "3              European/Caucasian-American                  [2-5]   \n",
       "4               'Latino/Hispanic American'                  [2-5]   \n",
       "\n",
       "  d_importance_same_religion field d_pref_o_attractive d_pref_o_sincere  \\\n",
       "0                      [2-5]   Law            [21-100]          [16-20]   \n",
       "1                      [2-5]   Law            [21-100]           [0-15]   \n",
       "2                      [2-5]   Law             [16-20]          [16-20]   \n",
       "3                      [2-5]   Law            [21-100]           [0-15]   \n",
       "4                      [2-5]   Law            [21-100]           [0-15]   \n",
       "\n",
       "  d_pref_o_intelligence  ... d_concerts d_music d_shopping d_yoga  \\\n",
       "0               [16-20]  ...     [9-10]  [9-10]      [6-8]  [0-5]   \n",
       "1                [0-15]  ...     [9-10]  [9-10]      [6-8]  [0-5]   \n",
       "2               [16-20]  ...     [9-10]  [9-10]      [6-8]  [0-5]   \n",
       "3                [0-15]  ...     [9-10]  [9-10]      [6-8]  [0-5]   \n",
       "4               [16-20]  ...     [9-10]  [9-10]      [6-8]  [0-5]   \n",
       "\n",
       "  d_interests_correlate d_expected_happy_with_sd_people  \\\n",
       "0              [0-0.33]                           [0-4]   \n",
       "1              [0.33-1]                           [0-4]   \n",
       "2              [0-0.33]                           [0-4]   \n",
       "3              [0.33-1]                           [0-4]   \n",
       "4              [0-0.33]                           [0-4]   \n",
       "\n",
       "  d_expected_num_interested_in_me d_expected_num_matches d_like  \\\n",
       "0                           [0-3]                  [3-5]  [6-8]   \n",
       "1                           [0-3]                  [3-5]  [6-8]   \n",
       "2                           [0-3]                  [3-5]  [6-8]   \n",
       "3                           [0-3]                  [3-5]  [6-8]   \n",
       "4                           [0-3]                  [3-5]  [6-8]   \n",
       "\n",
       "  d_guess_prob_liked  \n",
       "0              [5-6]  \n",
       "1              [5-6]  \n",
       "2              [0-4]  \n",
       "3              [5-6]  \n",
       "4              [5-6]  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse each column to numeric (after replacing missing values with -1) if possible\n",
    "def maybe_convert_to_int(col):\n",
    "    try:\n",
    "        col = pd.to_numeric(col.replace('?', -1), errors='raise').astype(int)\n",
    "    except Exception as e:\n",
    "        \n",
    "        return col\n",
    "    return col\n",
    "df = df.apply(maybe_convert_to_int, axis=0)\n",
    "df.select_dtypes(exclude=int).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# column selectors\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "cat_cols = make_column_selector(dtype_include=object)\n",
    "num_cols = make_column_selector(dtype_include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "num_pipe = make_pipeline(\n",
    "    # 1. Imputation\n",
    "    SimpleImputer(missing_values=-1, strategy='mean'),\n",
    "    # 2. Standardization\n",
    "    StandardScaler()\n",
    ")\n",
    "cat_pipe = make_pipeline(\n",
    "    # 3. Vectorization of categorical values\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    ")\n",
    "\n",
    "transform = make_column_transformer(\n",
    "    (num_pipe, num_cols), \n",
    "    (cat_pipe, cat_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We split the sample into a train and a test dataset. Only the train dataset will be used in the following exploratory analysis. This is a way to emulate a real situation where predictions are performed on an unknown target, and we don’t want our analysis and decisions to be biased by our knowledge of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"908f98ab-1eb2-4e35-b99f-0578606c0473\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"908f98ab-1eb2-4e35-b99f-0578606c0473\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(missing_values=-1)),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d940>),\n",
       "                                ('pipeline-2',\n",
       "                                 Pipeline(steps=[('ordinalencoder',\n",
       "                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n",
       "                                                                 unknown_value=nan))]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d7f0>)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bea2fd28-4da8-447b-bcfc-646ae9363b59\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bea2fd28-4da8-447b-bcfc-646ae9363b59\">pipeline-1</label><div class=\"sk-toggleable__content\"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d940></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b5a0473e-358d-4bc6-a8d6-acbac9f101e7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b5a0473e-358d-4bc6-a8d6-acbac9f101e7\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(missing_values=-1)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5b0f1f90-4394-4876-ab02-444013be0d1e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5b0f1f90-4394-4876-ab02-444013be0d1e\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b7a0626f-2c51-4b7c-b689-927126ed1410\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b7a0626f-2c51-4b7c-b689-927126ed1410\">pipeline-2</label><div class=\"sk-toggleable__content\"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d7f0></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a7f9ca8a-8191-487c-a370-b69d70f9897a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a7f9ca8a-8191-487c-a370-b69d70f9897a\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=nan)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(missing_values=-1)),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d940>),\n",
       "                                ('pipeline-2',\n",
       "                                 Pipeline(steps=[('ordinalencoder',\n",
       "                                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n",
       "                                                                 unknown_value=nan))]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7ff04de0d7f0>)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split + transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.drop(['decision', 'decision_o'], axis=1)\n",
    "df = df[num_cols(df) + cat_cols(df)]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df.drop('match', axis=1), df['match'])\n",
    "\n",
    "# nice display\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "transform.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.78136330e-01,  4.38303882e-01, -1.07009732e-01,  7.59540472e-01,\n",
       "       -2.55644504e-01, -8.05655860e-01, -9.79618943e-01, -9.38984297e-01,\n",
       "        2.05814840e-01, -3.25507131e-01, -3.23861053e-02,  4.40095569e-01,\n",
       "       -1.01697545e-01, -2.85632717e-01, -8.14764447e-02, -9.00003089e-02,\n",
       "        4.19967221e-01, -2.06480960e-01,  1.31754675e-01, -7.27523065e-01,\n",
       "       -1.99799136e-01, -1.03283214e+00,  7.14785276e-01,  1.24773287e+00,\n",
       "       -8.88513303e-02, -2.82807045e-01,  6.61451631e-01, -9.49444094e-01,\n",
       "        1.89496784e-01, -3.72681213e-01, -3.28797191e-01,  4.07850155e-01,\n",
       "       -1.04067970e-01, -2.49501374e-01, -2.08664183e-01,  1.16886382e-01,\n",
       "        2.62581065e-01, -1.62830377e-01, -9.23510450e-01,  3.12218951e-01,\n",
       "        1.27452598e+00, -1.46876540e+00, -7.63337004e-01, -1.45643330e+00,\n",
       "        4.58626970e-02,  1.30076568e+00,  6.47495343e-01,  2.78070050e-01,\n",
       "       -7.93759681e-01,  4.54530917e-02, -1.32296542e+00, -1.59855376e+00,\n",
       "        5.35105596e-01, -4.96935213e-01,  0.00000000e+00, -2.96243424e-01,\n",
       "       -7.92341681e-16, -5.31042939e-01, -7.76407761e-02,  3.71569436e-01,\n",
       "       -1.87710665e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "        4.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.12000000e+02,\n",
       "        2.00000000e+00,  0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "        1.00000000e+00,  0.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        2.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.00000000e+00,  2.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.00000000e+00,  0.00000000e+00,  2.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  1.00000000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print transformed\n",
    "transform.transform(Xtrain)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Vectorization and Standardization\n",
    "Let us inspect the _sklearn.preprocessing.StandardScaler_ used above and compare it to other useful standardizers:\n",
    " 1. _sklearn.preprocessing.MaxAbsScaler_\n",
    " 2. _sklearn.preprocessing.MinMaxScaler_\n",
    " 3. _sklearn.preprocessing.RobustScaler_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Vectorization and Standardization\n",
    "### _sklearn.preprocessing.StandardScaler_ \n",
    "---\n",
    " - Computes $\\frac{x - \\mu}{\\sigma}$ \n",
    " \n",
    "It is almost always beneficial, if not crucial, to standardize data to have zero mean and unit variance, if that data would otherwise be on different scales. \n",
    "\n",
    "In particular, algorithms that involve computation of hyperplanes (SVMs, Perceptron, MLPs, ...) are very susceptible to data on different scales, as their geometry relies on data to be on the same scale. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1 Vectorization and Standardization\n",
    "### _sklearn.preprocessing.StandardScaler_ \n",
    "---\n",
    "**In short: you almost always want to use the _StandardScaler_ on your numerical data**\n",
    "\n",
    "Exceptions are:\n",
    " - sparse data (Use MaxAbsScaler)\n",
    " - data with many/large outliers (Use RobustScaler)\n",
    "\n",
    "Remark: If you are scaling your training data in the range $[0,1]$, consider scaling it to $[-1,1]$ instead, so that it is distributed around the origin.\n",
    "(c.f. http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Vectorization and Standardization\n",
    "### _sklearn.preprocessing.MaxAbsScaler_ \n",
    "---\n",
    "\n",
    "Use this one if you have sparse data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    "transformer = StandardScaler().fit(X)\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Vectorization and Standardization\n",
    "### _sklearn.preprocessing.RobustScaler_ \n",
    "---\n",
    "\"This Scaler removes the median and scales the data according to the quantile range\" (c.f. sklearn docs)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Interquartile_range#/media/File:Boxplot_vs_PDF.svg\n",
    "\n",
    "- Use this one if you have many/large outliers in your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62725005, -1.41421356,  0.46291005],\n",
       "       [-0.78406256,  0.70710678,  0.9258201 ],\n",
       "       [ 1.41131261,  0.70710678, -1.38873015]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RobustScaler Example\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "X = [[ 1., -2.,  2.],\n",
    "     [ -2.,  1.,  3.],\n",
    "    [ 40.,  1., -2.]]\n",
    "transformer = StandardScaler().fit(X)\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Feature selection\n",
    "---\n",
    "We can perform Feature Selection :\n",
    "- Using trained models\n",
    " - Select features based on a LinearSVC estimator (c.f. L1-Regularization for feature selection) using coef_ attribute\n",
    " - Select features based on a RandomForest (using feature_importances_ attribute)\n",
    "\n",
    "- Using Variance or score function\n",
    " - sklearn.feature_selection.VarianceThreshold\n",
    " - sklearn.feature_selection.SelectKBest / sklearn.feature_selection.SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Feature selection\n",
    "### Using Models\n",
    "---\n",
    "Try out LinearSVC.coef_ and RandomForest.feature_importances_ as 'scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# fit LinearSVC on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8539379474940334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SelectFromModel using LinearSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(transform.fit_transform(Xtrain), ytrain)\n",
    "\n",
    "lsvc.score(transform.transform(Xtest), ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce number of features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm = SelectFromModel(lsvc, prefit=True)\n",
    "\n",
    "Xnew = sfm.transform(transform.transform(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age' 'd_age' 'importance_same_religion' 'pref_o_intelligence'\n",
      " 'pref_o_funny' 'pref_o_ambitious' 'attractive_o' 'funny_o' 'ambitous_o'\n",
      " 'shared_interests_o' 'intellicence_important' 'funny_important'\n",
      " 'ambtition_important' 'funny' 'attractive_partner' 'funny_partner'\n",
      " 'ambition_partner' 'shared_interests_partner' 'dining' 'art' 'gaming'\n",
      " 'clubbing' 'tv' 'shopping' 'expected_num_interested_in_me'\n",
      " 'expected_num_matches' 'like' 'guess_prob_liked' 'met' 'd_d_age' 'race'\n",
      " 'race_o' 'd_importance_same_race' 'field' 'd_pref_o_attractive'\n",
      " 'd_pref_o_sincere' 'd_attractive_important' 'd_sincere' 'd_intelligence'\n",
      " 'd_funny' 'd_sports' 'd_exercise' 'd_movies' 'd_concerts' 'd_yoga'\n",
      " 'd_expected_happy_with_sd_people']\n"
     ]
    }
   ],
   "source": [
    "# rerun training on reduced feature set\n",
    "mask = sfm.get_support()\n",
    "selected_features = np.array(Xtrain.columns)[mask]\n",
    "\n",
    "# refit transformer on reduced dataset\n",
    "transform.fit(Xtrain[selected_features])\n",
    "\n",
    "# retrain classifier\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(Xnew, ytrain)\n",
    "\n",
    "# evaluate on test set\n",
    "lsvc.score(transform.transform(Xtest[selected_features]), ytest)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SelectFromModel using RandomForestClassifier: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 Feature extraction/creation\n",
    "---\n",
    "\n",
    " - sklearn.preprocessing.PolynomialFeatures\n",
    " - sklearn.preprocessing.FunctionTransformer\n",
    "\n",
    "Create new features and possibly concatenate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 Feature extraction/creation\n",
    "### sklearn.preprocessing.PolynomialFeatures\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.],\n",
       "       [ 1.,  2.,  3.,  6.],\n",
       "       [ 1.,  4.,  5., 20.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomials\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.array([[0, 1],\n",
    "              [2, 3],\n",
    "              [4 ,5]])\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)\n",
    "# only interactions\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 Feature extraction/creation\n",
    "### sklearn.preprocessing.FunctionTransformer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'length': 140, 'num_sentences': 3}]\n"
     ]
    }
   ],
   "source": [
    "# FunctionTransformer text example\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "text = \"This is some arbitPrary text. We use it to demonstrate scikit learn's FunctionTransformer API. The number of sentences in the text is three.\"\n",
    "def text_stats(document):\n",
    "    return [{'length': len(document), 'num_sentences': document.count('.')}]\n",
    "text_stats_transformer = FunctionTransformer(text_stats)\n",
    "print(text_stats_transformer.fit_transform(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.02585093, 46.05170186, 69.07755279])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  FunctionTransformer log transform example\n",
    "large_numbers = np.array([1e10, 1e20,1e30])\n",
    "def apply_log(large_numbers):\n",
    "    return np.log(large_numbers)\n",
    "log_transform = FunctionTransformer(apply_log)\n",
    "log_transform.fit_transform(large_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "---\n",
    "  1. Cross Validation\n",
    "  2. GridSearch & RandomSearch\n",
    "  3. score metrics (Classification/Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "### 1. Cross Validation\n",
    "---\n",
    "**Must have** to estimate generalization error.\n",
    "\n",
    "\n",
    "Motivation: What can go wrong with the following procedure?\n",
    " 1. Train until convergence on the training data\n",
    " 2. Evaluate model performance on test set\n",
    " 3. tweak estimator parameters and repeat until estimator performs \n",
    "    'optimally' on test set\n",
    "\n",
    "Answer: next slide \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "### 1. Cross Validation\n",
    "---\n",
    "\n",
    "Problem: Overfitting on the **test set**, i.e. information _leakage_\n",
    "\n",
    "Solution: Validation set\n",
    "\n",
    "Next Problem: reduction of data set size due to three splits\n",
    "\n",
    "Solution: **Cross-validation**!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "1. fit using $k-1$ folds as training data\n",
    "2. remaining fold is validation set\n",
    "3. repeat as many times as you have splits\n",
    "\n",
    "<img src=\"img/grid_search_cross_validation.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "---\n",
    "If $n_{splits}$ is the number of total splits (training episodes), then performance of the model is measured using $\\frac{1}{n_{splits}} \\sum_{i=1}^{n_{splits}} score(split_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Folding example: Stratified folds\n",
    "Note: Do not underestimate nuances of different Cross-validation techniques!\n",
    "<img src=\"img/stratified.png\" alt=\"Drawing\" style=\"width: 1024px;\"/>\n",
    "Another useful example is GroupKFold: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation using sklearn\n",
    " - **Cross-validation = Folding (or splitting) + Scoring**\n",
    "\n",
    "**Folding** is done by [cross-validation-generator](https://scikit-learn.org/stable/glossary.html#term-cross-validation-generator), e.g.\n",
    "KFold, StratifiedKFold, GroupKFold,...\n",
    " - have get_n_splits() returns number of splits\n",
    " - split() returns train_indices, test_indices and is iterable\n",
    "\n",
    "```python\n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    ">>> for train_index, test_index in kf.split(X):\n",
    "...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "```\n",
    "\n",
    "**Scoring** is done by [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) \n",
    "\n",
    "**Combine** them using either \n",
    "```python sklearn.model_selection.cross_val_score``` or ```python sklearn.model_selection.cross_validate```. The latter returns among other information, the individual splits scores and can use multiple metrics. The former can only use a single metric and only returns the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84009547, 0.8353222 , 0.8353222 , 0.82006369, 0.83598726])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "# Folding\n",
    "cv = KFold(n_splits=5)\n",
    "# classsifier\n",
    "clf = SVC().fit(transform.fit_transform(Xtrain), ytrain)\n",
    "# scoring\n",
    "scoring = make_scorer(accuracy_score)\n",
    "cross_val_score(clf, transform.transform(Xtrain), ytrain, cv=cv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "### 2. GridSearch & RandomSearch\n",
    "---\n",
    "Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define parametergrids\n",
    "param_grid = [\n",
    "  {'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 6]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# demo GridSearchCV (alternative is e.g. RandomizedSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "selected_features = ['attractive_o', 'funny_o', 'shared_interests_o', 'attractive_partner']\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=None, scoring=None)\n",
    "#transform.fit(Xtrain[selected_features])\n",
    "\n",
    "#clf.fit(transform.transform(Xtrain[selected_features]), ytrain)\n",
    "#print(clf.best_params_)\n",
    "#clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "### 3. Score Metrics - Classification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Score Metrics - Classification\n",
    "---\n",
    "<img src=\"img/precision.png\" alt=\"Drawing\" style=\"width: 1024px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.8571428571428571\n",
      "0.75\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare precision, recall, harmonic mean of them (F1)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "y_pred = [1,1,1,1]\n",
    "y_true = [1,1,1,0]\n",
    "metrics = [accuracy_score, f1_score, precision_score, recall_score]\n",
    "[print(metric(y_true, y_pred)) for metric in metrics]\n",
    "# use accuracy whenever you care about true positives and true negatives\n",
    "# use f1_score whenever you care about false positives and false negatives\n",
    "# Conclusion: Consider F1 for unbalanced datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Model Selection\n",
    "### 3. Score Metrics - Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create two label sets, one scaled version of other\n",
    "y_true_1 = [0., .2, .5, .8, 1.]\n",
    "y_true_2 = [y * 100 for y in y_true_1]\n",
    "\n",
    "y_pred_1 = [0.1, .21, .51, .81, 1.1]\n",
    "y_pred_2 = [y * 100 for y in y_pred_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004060000000000004\n",
      "40.60000000000006\n",
      "0.9701470588235294\n",
      "0.9701470588235294\n"
     ]
    }
   ],
   "source": [
    "# demo MSE is not scale invariant\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(mean_squared_error(y_true_1, y_pred_1))\n",
    "print(mean_squared_error(y_true_2, y_pred_2))\n",
    "\n",
    "print(r2_score(y_true_1, y_pred_1))\n",
    "print(r2_score(y_true_2, y_pred_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End - happy to see you in the exercise session!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
