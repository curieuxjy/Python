{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Unsupervised Learning - Clustering\n",
    "## Theory\n",
    "### Task 1: MC\n",
    "---\n",
    "Multiple answers are possible.\n",
    "\n",
    "**DBSCAN**\n",
    " - [ ] can be used for outlier detection\n",
    " - [ ] works best with isotropic data\n",
    " - [ ] can be used to construct a KDTree\n",
    " - [ ] computes all possible pairwise distances\n",
    "\n",
    "**K-means**\n",
    " - [ ] guarantees convergence\n",
    " - [ ] is insensitive to outliers\n",
    " - [ ] can be used with a custom distance function \n",
    " - [ ] All of the above\n",
    "\n",
    "**Agglomerative HC**\n",
    " - [ ] can be used with a custom distance function\n",
    " - [ ] can be used with categorical data\n",
    " - [ ] 's result can be used to visualize the distance matrix\n",
    " - [ ] All of the above\n",
    "\n",
    "**K-means**\n",
    " - [ ] is fast in practice\n",
    " - [ ] scales well with the dataset size\n",
    " - [ ] can be used for outlier detection\n",
    " - [ ] All of the above\n",
    "\n",
    "**K-medoids**\n",
    " - [ ] can be used with a custom distance function\n",
    " - [ ] can be initialized with kmeans++ initialization\n",
    " - [ ] computes centroids using the median\n",
    " - [ ] All of the above\n",
    "\n",
    "**Clustering Evaluation**\n",
    " - [ ] ```sklearn.metrics.silhouette_score``` is used to evaluate a clustering given the true (cluster) labels\n",
    " - [ ] ```sklearn.metrics.silhouette_score``` is used to evaluate a clustering given the data ```X```\n",
    " - [ ] ```sklearn.metrics.rand_score``` computes differences for all possible pairs of points\n",
    " - [ ] ```sklearn.metrics.rand_score``` can also be used for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "### Task 1: KMeans\n",
    "---\n",
    "For this task you are given data ```X, y``` that is distributed around four centers. \n",
    "The goal of this exercise is to familiarize yourself with the pitfalls of K-mean (and its evaluation performance). Therefore\n",
    "\n",
    "1. Plot X using a scatter plot, and color the data according to the labels\n",
    "2. run KMeans with cluster sizes from 2 to 6 and this time plot the data using color according to the  prediction. \n",
    "3. Place both of these plots next to each other by first calling \n",
    "```fig, (ax1, ax2) = plt.subplots(1, 2)``` and then placing each plot on one axis\n",
    "\n",
    "Follow this link for an example on how to use subplots (you have to sccroll down) https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.subplots.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,  # 2D data\n",
    "                  centers=4,  # distributed in 4 clusters\n",
    "                  cluster_std=1, # std deviation\n",
    "                  center_box=(-10.0, 10.0), # xlim, ylim\n",
    "                  shuffle=True,\n",
    "                  random_state=1)\n",
    "\n",
    "# Follow steps 1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be as expected. Now add a score evaluation using the ```sklearn.metrics.silhouette_score``` to each of kmeans predictions. \n",
    "\n",
    "**What can you conclude?**\n",
    "\n",
    "**[Optional]** Refine the plots, by including axis titles  'true_clusters' and 'predicted_clusters' on each corresponding plot. For the axis corresponding to 'predicted_clusters', also add the silhouette_score value to the title. the results should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\exercise_one.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment, but with a anisotropically scaled (1) version of the previous dataset X and using only the correct number of clusters (four), i.e. do not compute K-means for 2,3,5,6 clusters. \n",
    "\n",
    "**What can you conclude** after using K-means on this scaled version of the original data?\n",
    "\n",
    "(1) For that, use the transformation ```transform = np.array([[1.6, -0.1], [1,-.5]]) ``` and use ```numpy.dot``` together with the original data to obtain the transformed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# rescale and run K-means using 4 clusters, \n",
    "\n",
    "# plot and evaluate as before using\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Agglomerative Clustering\n",
    "---\n",
    "The goal of this exercise is to show how you can enhance hierarchical clustering by using a matrix of neighborhoods, that will restrict computation of pairwise distances to only the connected neighbors.\n",
    "\n",
    "For this exercise, you are given the ```S-curve``` dataset, which is used to show that clustering (or manifold-learning) algorithms preserve locality. We will see that in the default case, Agglomerative Clustering does not do so.\n",
    "\n",
    "\n",
    "For that, you should\n",
    "1. visualize the 3 dimensions of your data by calling ax.scatter3D.\n",
    "   As a color map use plt.cm.Spectral\n",
    "2. compute AgglomerativeClustering with n_clusters=8 and default linkage\n",
    "3. repeat the plot, this time coloring the data according to the labels returned by the clustering\n",
    "4. Now use AgglomerativeClustering again, but using the 'connectivity' argument. You should pass a ```kneighbors_graph``` of the data with ```n_neighbors=4```. By default, neighbors are computed using the euclidean distance. You should leave this default setting.\n",
    "5. repeat the plot with labels colored according to the new clustering.\n",
    "\n",
    "**What is different?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# initialize plots using\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.view_init(7, -80)\n",
    "\n",
    "# S-curve data\n",
    "X, t = make_s_curve(1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Tuning DBSCAN\n",
    "---\n",
    "Using what we have learned so far, find the combination of ```eps, min_samples``` that works well for the given dataset and explain what well could mean in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, y = make_blobs(n_samples=750, centers=centers, cluster_std=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
