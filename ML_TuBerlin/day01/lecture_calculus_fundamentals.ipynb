{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalar derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuition of derrivatives\n",
    "![Slope](resources/der.png)\n",
    "\n",
    "The derivative of a scalar-valued function is the rate of change/slope at a certain point, here at $f(x^*)$, or $x^*$. As shown the derivative function here is:\n",
    "$f'(x)=\\frac{\\partial y}{\\partial x}=\\frac{\\partial f(x)}{\\partial x}=\\frac{\\partial x^2}{\\partial x}=2x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $f$ maps from $\\mathbb{R}\\rightarrow \\mathbb{R}$, or $f(x)\\in\\mathbb{R}$, so the derrivative $\\frac{ \\partial f(x)}{\\partial x}$ has dimensionality $1$, or in general $\\frac{\\partial f(x)}{\\partial x}\\in\\mathbb{R}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An important aspect of derivatives:\n",
    "Intuitively the derivative of a function $f(x)$ with respect to $x$ measures how an infinitesimal change in $x$ affects the value of $f(x)$. Derivatives evaluated at a certain point in the input space always point in the direction of steepest ascent. As an example consider the above derivative function $f'(x)$, evaluated at $x=2$. We get $f'(2)=2\\cdot2=4$, which is a positive number that can be interpreted as pointing to the positive direction(seen from point 0). Thus by increasing our input value $x_1$ by f'(2), or some smaller number, should result in a higher function value $f(x)$. And indeed we get $f(2+4)=f(6)=36 > f(2)=4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivation rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Constang rule:\n",
    "- Computing the derivative of a constant function is zero. \n",
    "- For example the derivative of $f(x)=2$, is $f'(x)=\\frac{\\partial f(x)}{\\partial x}=\\frac{\\partial 2}{\\partial x}=0$. This makes senese, since the slope of a constant function is zero at every point in input space(x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5,5,50)\n",
    "y = np.ones(len(x)) * 2\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sum rule:\n",
    "| $f(x)$ | $g(x)+h(x)$ | $2x^2 + 7x$ |\n",
    "|:- |:- |:- |\n",
    "| $f'(x)$ | $g'(x)+h'(x)$ | $4x + 7$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Chain rule:\n",
    "| $f(u)$ | $f(g(x))$ | $(x+1)^2$ |\n",
    "|:- |:- |:- |\n",
    "| $f'(x)$ | $f'(u)\\cdot g'(x)$ | $1\\cdot2(x+1)$ |\n",
    "\n",
    "where in the last example we have $u:=g(x)=x+1$, and $f(u)=u^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Rewriting the chain rule as follows immediately clarifies why the above rule can be applied:\n",
    "    - Let $y=f(u)$, and $u=g(x)$ or, $f(u):=f(g(x))$\n",
    "    - We can write $\\frac{\\partial f(u)}{\\partial x}=\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u}\\cdot \\frac{\\partial u}{\\partial x}=f'(u) \\cdot g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Product rule:\n",
    "| $f(x)$ | $g(x)h(x)$ | $(x+1)^2x$ |\n",
    "|:- |:- |:- |\n",
    "| $f'(x)$ | $g'(x)h(x)+g(x)h'(x)$ | $2(x+1)x+1(x+1)^2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of common scalar derivative functions:\n",
    "| $f(x)$ | $x^n$ | $\\exp(x)$ | $\\log(x)$ | $\\sin(x)$ | $\\cos(x)$ | $\\sigma(x):=\\frac{1}{1+\\exp(-x)}$ |   \n",
    "|:- |:- |:- |:- |:- |:- |:- |\n",
    "| $f'(x)$ | $nx^{n-1}$ | $\\exp(x)$ | $\\frac{1}{x}$ | $\\cos(x)$ | $-\\sin(x)$ | $\\sigma(x)(1-\\sigma(x))$ |\n",
    "The last derivative function will be used quite extensively, later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partial derivatives and gradient\n",
    "What if our function depends on more than one variable ? E.g.:\n",
    "- $f(x_1,x_2)=x_1^2+x_2^2$\n",
    "![Partial](resources/paraboloid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The derivative of above scalar-valued function is a vector, and has the following form:\n",
    "    - Let $y=f(x_1,x_2)$\n",
    "    - If we want to compute the derivative of the function $y=f(x_1,x_2)$ with respect to only one variable, say $x_1$, it is as simple as computing $\\frac{\\partial y}{\\partial x_1}$, while treating the other variable, here $x_2$ as a constant.\n",
    "    - Intuitively this partial derivative wrt. $x_1$, just like in the scalar case, can be interpreted as how a small change in $x_1$ affects our function value $f(x_1,x_2)$. It also tells us, how to change, or ***optimize*** this variable (increase or decrease it), in order to ***increase the value of the outcome*** of $y=f(x)$, as quick as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Computing the partial derivative of a scalar-valued function with respect to all input variables, $(x_1,x_2)$(can be extended to an n-dimensional vector), leads to the following vector. Let $y=f(x_1,x_2)=x_1^2+x_2^2$. We get: $\\frac{\\partial y}{\\vec{x}}=\\Biggl[\\begin{matrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\frac{\\partial y}{\\partial x_2}\\end{matrix}\\Biggl]=\\Biggl[\\begin{matrix} 2x_1 \\\\ 2x_2\\end{matrix}\\Biggl]$. This is also called the ***gradient*** of the function $f(x_1,x_2)$ and is often denoted in literatur as $\\nabla f(x_1,x_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Geometric interpretation of the gradient\n",
    "![Partial_derrivative](resources/pd.png)\n",
    "***Important take-aways:***\n",
    "The gradient of a function, evaluated at a certain point in the input space(here $x_1$ and $x_2$), is the vector that is perpendicular to the tangent that passes through that point on the isocontour line. For a scalar-valued function that depends on two-dimensional variables it points in the direction of steepest ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### N-dimensional arrays\n",
    "- Of course the theory of partial derivatives can be easily extended to scalar-valued functions $y=f(x_1,...,x_n)$ that depend on n-dimensional input vectors $\\vec{x}\\in\\mathbb{R}^{n}$.\n",
    "- The gradient with respect to the vector $\\vec{x}=\\Biggl[\\begin{matrix} x_1 \\\\ ... \\\\ x_n \\end{matrix}\\Biggl]$ is simply $\\frac{\\partial y}{\\partial \\vec{x}}=\\Biggl[\\begin{matrix} \\frac{\\partial y}{\\partial x_1} \\\\ ... \\\\ \\frac{\\partial y}{\\partial x_n} \\end{matrix}\\Biggl]$.\n",
    "- So the derrivative $\\frac{\\partial y}{\\partial \\vec{x}}$ has dimensionality $n$, or more general $\\frac{\\partial y}{\\partial \\vec{x}}\\in\\mathbb{R}^{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can easily apply the partial derivative rules,(derive function with respect to one variable and treat the others as constant) to the matrix case. \n",
    "- In particular, given a scalar-valued function $y=f(x_{11},...,x_{nm})$ and a matrix $X\\in\\mathbb{R}^{n,m}$, with entries: $\\Biggl[\\begin{matrix} x_{11} &...& x_{1m} \\\\ ...& ... & ... \\\\ x_{n1} &...& x_{nm}\\end{matrix}\\Biggl]$, we can compute the gradient of that function with respect to $X$ as follows: $\\frac{\\partial y}{\\partial X}=\\Biggl[\\begin{matrix} \\frac{\\partial y}{\\partial x_{11}} &...& \\frac{\\partial y}{\\partial x_{1m}} \\\\ ...& ... & ... \\\\ \\frac{\\partial y}{\\partial x_{n1}} &...& \\frac{\\partial y}{\\partial x_{nm}}\\end{matrix}\\Biggl]$.\n",
    "- Each partial derivative in that matrix can be interpreted as, what ***impact*** each of the variables has on the outcome of $y$, at a particular point $(x_{11},...,x_{nm})$. It also tells us, how to change, or ***optimize*** each variable (increase or decrease it), in order to ***increase the value of the outcome*** of $y$, as quick as possible.\n",
    "- So the derrivative $\\frac{\\partial y}{\\partial X}$ has dimensionality $n x m$, or more general $\\frac{\\partial y}{\\partial X}\\in\\mathbb{R}^{n,m}$. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
