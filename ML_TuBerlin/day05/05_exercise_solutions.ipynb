{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Support Vector Machines and RandomForests\n",
    "## Theory\n",
    "### Task 1: Derivation of the dual problem\n",
    "---\n",
    "Derive the Dual formulation of the SVM optimization problem. I.e.\n",
    "\n",
    "Given \n",
    " - $\\underset{\\alpha}{max}\\big\\{ \\underset{w,b}{min}\\{ \\frac{1}{2}\\lVert w\\rVert^2 + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b)) \\} \\big\\}, \\alpha_i \\geq 0,$ for all $i=1,...,m$,\n",
    " \n",
    "show that a solution can be obtained by\n",
    " - $\\underset{\\alpha}{max} \\sum_i \\sum_j \\alpha_i - \\frac{1}{2} \\alpha_i\\alpha_jy_iy_j\\langle\\Phi(x_i), \\Phi(x_j)\\rangle$ \n",
    "\n",
    "subject to the constraints $\\mathbf{\\alpha} \\geq 0 $ and $\\sum_i\\alpha_iy_i = 0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "---\n",
    "Starting from\n",
    " - $\\underset{\\alpha}{max}\\big\\{ \\underset{w,b}{min}\\{ \\frac{1}{2}\\lVert w\\rVert^2 + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b)) \\} \\big\\}, \\alpha_i \\geq 0,$ for all $i=1,...,m$,\n",
    " \n",
    "we first set the derivative of the inner term to zero, in order to minimize the inner problem analytically.\n",
    "\n",
    "$\\frac{\\partial \\frac{1}{2}\\lVert w\\rVert^2 + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b))}{\\partial w} = w - \\sum_{i=1}\\alpha_i y_i \\Phi(x_i) \\overset{!}{=} 0$ \n",
    "\n",
    "$\\implies w = \\sum_{i=1}\\alpha_i y_i \\Phi(x_i)$\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\frac{1}{2}\\lVert w\\rVert^2 + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b))}{\\partial b} = - \\sum_{i=1}\\alpha_i y_i  \\overset{!}{=} 0$ \n",
    "\n",
    "$\\implies \\sum_{i=1}\\alpha_i y_i = 0$\n",
    "\n",
    "Plugging in the primal variable $w$ into the original inner minimization problem, we obtain\n",
    "\n",
    " - $\\frac{1}{2}\\lVert w\\rVert^2 + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b)) $\n",
    "\n",
    "$ = w^Tw + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b))$\n",
    "\n",
    "$ = \\frac{1}{2} \\underbrace{\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_j y_iy_j \\Phi(x_i)^T\\Phi(x_j)}_{w^Tw} + \\sum_{i=1} \\alpha_i(1 - y_i(w^T\\Phi(x_i) + b))$\n",
    "\n",
    "$ = \\frac{1}{2} \\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_j y_iy_j \\Phi(x_i)^T\\Phi(x_j)_{w^Tw} - \\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_j y_iy_j \\Phi(x_i)^T\\Phi(x_j)$\n",
    "\n",
    "$ = -\\frac{1}{2} \\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_j y_iy_j \\Phi(x_i)^T\\Phi(x_j)_{w^Tw}$\n",
    "\n",
    "subject to the constraints \n",
    " - $\\alpha_i \\geq 0,$ for all $i=1,...,m$\n",
    "given by the original optimization problem, and the contraints\n",
    " - $\\sum_{i=1}\\alpha_i y_i = 0$ from the derivate w.r.t $b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Primal variables w,b from the dual solution\n",
    "---\n",
    "Show how the parameters $w$ and $b$ can be obtained from the solution of dual for predicting new points. Note: $b$ is tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "---\n",
    "Show how the parameters $w$ and $b$ can be obtained from the solution of dual for predicting new points. Note: $b$ is tricky\n",
    "\n",
    " - $w = \\sum_{i=1}\\alpha_i y_i \\Phi(x_i)$ is obtained directly from the dual derivation.\n",
    "\n",
    "To obtain $b$, observe that, for support vectors, we have   \n",
    " - $\\underset{i}{min} \\{y_i(w^t\\Phi(x_i)+b \\} = 1 = \\underset{i_{|y_i=1}}{min} \\{y_i(w^t\\Phi(x_i)+b \\}$, \n",
    " \n",
    "where the last minimization is just over y_i=1, so that it is equal to \n",
    " \n",
    " - $\\underset{i_{|y_i=1}}{min} \\{w^t\\Phi(x)+b \\}$\n",
    " \n",
    "Then we get by pulling $b$ out of the minimization, as it is a constant, and plugging in $w =  \\sum_{i=1}\\alpha_i y_i \\Phi(x_i)$ that \n",
    " - $b = 1 - \\underset{i_{|y_i=1}}{min} \\{\\sum_j \\alpha_jy_j\\Phi(x_j)^T\\Phi(x_i)+b \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Hinge loss\n",
    "---\n",
    "The hinge loss is defined as \n",
    " - $l_{hinge}(y, g(x)) = max(0, 1-yg(x))$\n",
    "\n",
    "Show how we can inject it into the Soft Margin Primal Problem\n",
    "- $\\underset{w,b,\\xi}{min} \\frac{1}{2}\\lVert w \\rVert^2 + C \\sum_i \\xi_i$ s. t.\n",
    "\n",
    "$y_i(w^T\\Phi(x_i) + b) \\geq 1 - \\xi_i$ and $\\xi_i \\geq 0$.\n",
    "\n",
    "to remove the constraints completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "---\n",
    "Define $g(x) = w^T\\Phi(x) + b$. Then \n",
    " - $\\xi_i \\geq  1- y_i\\cdot g(x_i)\n",
    " \\Leftrightarrow \\xi_i = max(0, 1-y\\cdot g(x)) =: l_{hinge},$\n",
    "\n",
    "because $\\xi_i$ is at most $0$.\n",
    "\n",
    "So now we can replace $\\xi_i$ in the objective function with $l_{hinge}$, and thereby incorporate the constraints in the objective itself:\n",
    " - $\\underset{w,b,\\xi}{min} \\frac{1}{2}\\lVert w \\rVert^2 + C \\sum_i l_{hinge}$\n",
    " \n",
    "The implication is, L2 regularization can be geometrically interpreted as the penalty for small margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Sklearn Warmup using toy datasets\n",
    "---\n",
    "**Task 1 SVC**\n",
    " - Train one sklearn.svm.SVC (Support Vector Classifier) model on the wine-dataset for each predefined kernel.\n",
    " - You can get the dataset via sklearn.datasets.load_wine.\n",
    " - Split the dataset into train and test data and report the kernel that performs best on the test data using the sklearn.metrics.accuracy_score metric\n",
    " - plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification using SVC on the load_wine dataset\n",
      "accuracy_score = 0.7555555555555555, kernel = rbf\n",
      "accuracy_score = 0.9555555555555556, kernel = linear\n",
      "accuracy_score = 0.7111111111111111, kernel = poly\n",
      "[[19  0  0]\n",
      " [ 0 13  2]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def do_exercise_one(load_fn):\n",
    "  Xtrain, Xtest, ytrain, ytest = train_test_split(*load_fn(return_X_y=True), shuffle=True, random_state=12)\n",
    "  print(f'Classification using SVC on the {load_fn.__name__} dataset')\n",
    "  for kernel in ['rbf', 'linear', 'poly']:\n",
    "    clf = OneVsRestClassifier(SVC(kernel=kernel)).fit(Xtrain, ytrain)\n",
    "    ypred = clf.predict(Xtest)\n",
    "    acc = accuracy_score(ypred, ytest)\n",
    "    print(f'accuracy_score = {acc}, kernel = {kernel}')\n",
    "    assert acc == clf.score(Xtest, ytest)\n",
    "  # plot confusion_matrix\n",
    "  ypred = OneVsRestClassifier(SVC(kernel='linear')).fit(Xtrain, ytrain).predict(Xtest)\n",
    "  \n",
    "  print(confusion_matrix(ytest, ypred))\n",
    "\n",
    "do_exercise_one(load_wine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Warmup using toy datasets:\n",
    "---\n",
    "**Task 1 RandomForest**\n",
    " - Train one sklearn.ensemble.RandomForestRegressor model on the diabetes-dataset for each predefined criterion.\n",
    " - You can get the dataset via sklearn.datasets.load_diabetes.\n",
    " - Split the dataset into train and test data and report the criterion that performs best on the test data using the sklearn.metrics.r2_score metric\n",
    " - plot the predicted against the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting RandomForestRegressor on diabetes dataset\n",
      "finished fitting\n",
      "R2 = -0.011634657673644133\n",
      "score = 0.4755142058523225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_exercise_two():\n",
    "  Xtrain, Xtest, ytrain, ytest = train_test_split(*load_diabetes(return_X_y=True), shuffle=True, random_state=42)\n",
    "  print('fitting RandomForestRegressor on diabetes dataset')\n",
    "  clf = RandomForestRegressor().fit(Xtrain, ytrain)\n",
    "  print('finished fitting')\n",
    "  ypred = clf.predict(Xtest)\n",
    "  acc = r2_score(ypred, ytest)\n",
    "  print(f'R2 = {acc}')\n",
    "  print(f'score = {clf.score(Xtest, ytest)}')\n",
    "  plt.scatter(ytest, ypred)\n",
    "  plt.show()\n",
    "\n",
    "do_exercise_two()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the warmup, you do not have to do any preprocessing. We will cover data preprocessing(standardization, feature selection, etc.) in the next lecture.\n",
    "\n",
    "Similarly, you do not have to use cross validation or any sort of hyperparameter selection. You can for now use the default values of the estimators.\n",
    "The according methods will be covered in the next lecture as well.\n",
    "\n",
    "If your SVM is slow and your dataset not too big, scale data to [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Multiclass classification\n",
    "---\n",
    "Apply the SVM to a multiclass classification problem of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: EEG Eye state dataset (Time series data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will \n",
    "do classification on the EEG Eye state dataset. Sliding windows have already been applied to turn it into a supervised problem. You can familiarise yourself with it here: https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State\n",
    "\n",
    "1. Load the data and labels and transform them into X, y in the format of sklearn toy datasets. (Tricky)\n",
    "\n",
    "```python\n",
    "from scipy.io import arff\n",
    "with open('data/eyes.arff', mode='r') as f:\n",
    "    # X, y = ...\n",
    "    # Xtrain, Xtest, ytrain, ytest = ... \n",
    "    pass\n",
    "```\n",
    "2. Fit a RandomForest Classifier and SVC on the data after splitting it into train- and testset.\n",
    "3. Report the best result. Can you tune the SVC to reach the performance of the random forest with default params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and compute train test splits\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('data/eyes.arff', mode='r') as f:\n",
    "  data, meta = arff.loadarff(f)\n",
    "  n_samples = len(data)\n",
    "  n_features = len(data[0])-1\n",
    "  X = np.zeros((n_samples, n_features))\n",
    "  y = np.zeros((n_samples,))\n",
    "  for i, row in enumerate(data):\n",
    "    X[i,:], y[i] = list(row)[:-1], row[-1]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fitting\n"
     ]
    }
   ],
   "source": [
    "# fit classifiers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "clf = SVC().fit(Xtrain, ytrain)\n",
    "print('done fitting')\n",
    "ypred_svc = clf.predict(Xtest)\n",
    "forest = RandomForestClassifier().fit(Xtrain, ytrain)\n",
    "ypred_rf = forest.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc SVC = 0.7081441922563418\n",
      "acc RF= 0.9188251001335114\n"
     ]
    }
   ],
   "source": [
    "# compare results, maybe tune SVC\n",
    "print(f'acc SVC = {accuracy_score(ypred_svc, ytest)}')\n",
    "print(f'acc RF= {accuracy_score(ypred_rf, ytest)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Novelty detection using SVM\n",
    "---\n",
    "Novelty detection inspired by SVMs is done by finding the smalles enclosing sphere of the data, i.e. solving\n",
    " - $\\underset{R,c}{min} R^2$ subject to the constraints\n",
    " $\\lVert \\Phi(x_i)-c\\rVert^2 \\leq R^2$\n",
    "\n",
    "A point $x$ is then reported a novelty, if it exceeds a distance from the center:\n",
    "$f(x) = sign(\\lVert \\Phi(x_i)-c\\rVert - \\tau)$\n",
    " \n",
    "Familiarise yourself with the section on novelty detection via svms, by reading the corresponding section in the sklearn userguide\n",
    " - https://scikit-learn.org/stable/modules/outlier_detection.html#novelty-detection\n",
    "\n",
    "Pick a dataset from the Outlier Detection DataSets http://odds.cs.stonybrook.edu/ and try to find the outliers using sklearn.svm.OneClassSvm\n",
    "\n",
    "You can report your findings, either by comparison or using plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best performance: {'kernel': 'sigmoid', 'nu': 0.5}\n",
      "final f1_score = 0.782608695652174 with recall = 1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "content_dict = loadmat('./data/glass.mat')\n",
    "X, y = content_dict['X'],  content_dict['y']\n",
    "Xtrain, Xtest, ytrain, ytest = X[:175], X[175:], y[:175], y[175:]\n",
    "\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "nus = [.01, .1, .5, .9, .99]\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "best_params = dict()\n",
    "best_f1_score = -np.inf\n",
    "recall = np.inf\n",
    "for kernel in kernels:\n",
    "  for nu in nus:\n",
    "    clf = OneClassSVM(nu=nu,kernel=kernel).fit(Xtrain, ytrain)\n",
    "    vals = clf.decision_function(scaler.transform(Xtest))\n",
    "    scores = clf.score_samples(scaler.transform(Xtest))\n",
    "\n",
    "    vals = np.abs(vals)\n",
    "    pred = clf.predict(scaler.transform(Xtest)).clip(min=0)  # replace -1 with 0 to be able to compute metrics\n",
    "    current_f1_score = f1_score(ytest, pred)\n",
    "    if current_f1_score > best_f1_score:\n",
    "      best_params = {'kernel': kernel, 'nu': nu}\n",
    "      best_f1_score = current_f1_score\n",
    "      recall = recall_score(ytest, pred)\n",
    "\n",
    "print(f'best performance: {best_params}')\n",
    "print(f'final f1_score = {best_f1_score} with recall = {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
